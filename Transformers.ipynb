{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imycnIYuxeBB",
        "outputId": "3737d6fe-6491-4836-fb08-995be8b8eb6e"
      },
      "source": [
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch version: 1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LU75nlTy60M"
      },
      "source": [
        "# **Attention Is All You Need (Transformer)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN5eDUvmzPS4"
      },
      "source": [
        "## **Core Idea of the Paper**\n",
        "\n",
        "### ***Problem***\n",
        "*   In sequence-to-sequence problems such as the neural machine translation, the first proposals were based on the use of RNNs in an encoder-decoder architecture.\n",
        "*    The best performing models also connect the encoder and decoder through an attention mechanism.\n",
        "*   These architectures have a great limitation when working with long sequences\n",
        "*   In the encoder, the hidden state in every step is associated with a certain word in the input sentence, usually one of the most recent. Therefore, if the decoder only accesses the last hidden state of the decoder, it will lose relevant information about the first elements of the sequence.\n",
        "\n",
        "### **Solution**\n",
        "\n",
        "*   This paper propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms to draw global dependencies between input and output.\n",
        "*   Instead of paying attention to the last state of the encoder as is usually done with RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence.\n",
        "*   The total computational complexity per layer\n",
        "*   The amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
        "*   The path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks.\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41rcSLp8RsbJ"
      },
      "source": [
        "## ***Model Architecture***\n",
        "\n",
        "<h4 align=\"center\">The Transformer - model architecture.</h4>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1Q9u7Elc7bbk69cd4ToG2F199ZuweP1s1\" width=\"400\" height=\"500\">\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfL6zXDQDJq1"
      },
      "source": [
        "###   ***Input Embedding and Softmax***\n",
        "\n",
        "With this layer, we convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$ using a learned embedding. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$.                                                 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0upa-XH6EgwZ"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.lut = nn.Embedding(vocab, d_model)\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIVTSUBU0TEl"
      },
      "source": [
        "###   ***Positional Encoding***\n",
        "\n",
        "In order for the model to make use of the\n",
        "order of the sequence, we must inject some information about the relative or absolute position of the tokens in the senquence.\n",
        "In this work, we use sine and cosine functions of different frequencies:\n",
        "\n",
        "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
        "\n",
        "$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$    \n",
        "\n",
        "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized **it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvqS66HF92AS"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        PE = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        PE[:, 0::2] = torch.sin(position * div_term)\n",
        "        PE[:, 1::2] = torch.cos(position * div_term)\n",
        "        PE = PE.unsqueeze(0)\n",
        "        self.register_buffer('PE', PE)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.PE[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtPfX-3ZNDEb"
      },
      "source": [
        "###   ***Scaled Dot-Product Attention***\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1s6mn-NtXM0ux0KaCmacE6QBBmnHOglgW\" width=\"400\" height=\"300\">\n",
        "</center>\n",
        "\n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
        "\n",
        "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      \n",
        "                                                                 \n",
        "$$                                                                         \n",
        "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
        "$$   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn4MkdE1OTOV"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "  def __init__(self, d_k, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_k = d_k\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "\n",
        "    attn = torch.matmul(q / math.sqrt(self.d_k), k.transpose(2, 3))\n",
        "\n",
        "    if mask is not None:\n",
        "        attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attn = self.dropout(F.softmax(attn, dim=-1))\n",
        "    output = torch.matmul(attn, v)\n",
        "\n",
        "    return output, attn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_2EkjjQ4R3"
      },
      "source": [
        "###   ***Multi-Head Attention***\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1uKAOwRZ6bL48WoAVar9ku06-sgjRCsi5\" width=\"400\" height=\"300\">\n",
        "</center>\n",
        "\n",
        "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.                                            \n",
        "$$    \n",
        "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
        "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
        "$$                                                                                                                 \n",
        "\n",
        "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.                                          "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2yHY9enRkOo"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  ''' Multi-Head Attention module '''\n",
        "\n",
        "  def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_head = n_head\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, n_head * d_k)\n",
        "    self.w_k = nn.Linear(d_model, n_head * d_k)\n",
        "    self.w_v = nn.Linear(d_model, n_head * d_v)\n",
        "    self.FC = nn.Linear(n_head * d_v, d_model)\n",
        "\n",
        "    self.attention = ScaledDotProductAttention(d_k)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layerNorm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "\n",
        "    residual = Q\n",
        "\n",
        "    # Pass through the pre-attention projection: b x lq x (n*dv)\n",
        "    # Separate different heads: b x lq x n x dv\n",
        "    Q = self.w_q(Q).view(Q.size(0), Q.size(1), self.n_head, self.d_k)\n",
        "    K = self.w_k(K).view(K.size(0), K.size(1), self.n_head, self.d_k)\n",
        "    V = self.w_v(V).view(V.size(0), V.size(1), self.n_head, self.d_v)\n",
        "\n",
        "    # Transpose for attention dot product: b x n x lq x dv\n",
        "    Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
        "\n",
        "    Q, attn = self.attention(Q, K, V, mask=mask)\n",
        "\n",
        "    # Transpose to move the head dimension back: b x lq x n x dv\n",
        "    # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
        "    Q = Q.transpose(1, 2).contiguous().view(Q.size(0), Q.size(1), -1)\n",
        "    Q = self.FC(Q)\n",
        "    Q = self.dropout(Q)\n",
        "\n",
        "    return Q, attn"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5yl-1eGW4RA"
      },
      "source": [
        "###   ***Position-wise Feed-Forward Networks***\n",
        "\n",
        "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                       "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ublkNpBXkJX"
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  ''' A two-feed-forward-layer module '''\n",
        "\n",
        "  def __init__(self, d_model, d_hid, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.W_1 = nn.Linear(d_model, d_hid)\n",
        "    self.W_2 = nn.Linear(d_hid, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.W_1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.W_2(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykwJ2AZ9bIMT"
      },
      "source": [
        "###   ***Built our Encoder***\n",
        "\n",
        "Since the core encoder contains N=6 encoder sublayer, we will define an EncoderLayer class for each sublayer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3aUZJlSesea"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  ''' Compose with two layers '''\n",
        "\n",
        "  def __init__(self, d_model, d_hid, n_head, d_k, d_v, dropout=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.MHA = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "    self.layerNorm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "    self.FFN = PositionwiseFeedForward(d_model, d_hid, dropout=dropout)\n",
        "\n",
        "  def forward(self, encoder_input, mask=None):\n",
        "    res1 = encoder_input\n",
        "    output1, MHA = self.MHA(encoder_input, encoder_input, encoder_input, mask=mask)\n",
        "    output1 = res1 + self.layerNorm(output1)\n",
        "    res2 = output1\n",
        "    output2 = self.FFN(output1)\n",
        "    output2 = res2 + self.layerNorm(output2)\n",
        "    encoder_output = output2\n",
        "    return encoder_output, MHA"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFN3MhBUin2V"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"Core encoder is a stack of N layers\"\n",
        "  def __init__(self, layer, N):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
        "    self.layerNorm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        \n",
        "  def forward(self, x, mask):\n",
        "    \"Pass the input (and mask) through each layer in turn.\"\n",
        "    for layer in self.layers:\n",
        "        x = layer(x, mask)\n",
        "    return self.layerNorm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOTW83ZlTK6g"
      },
      "source": [
        "###   ***Built our Decoder***\n",
        "\n",
        "Since the core decoder contains N=6 decoder sublayer, we will define an DecoderLayer class for each sublayer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rII6qbgVPIX"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  ''' Compose with three layers '''\n",
        "\n",
        "  def __init__(self, d_model, d_hid, n_head, d_k, d_v, dropout=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.MMHA = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "    self.MHA = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "    self.layerNorm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "    self.FFN = PositionwiseFeedForward(d_model, d_hid, dropout=dropout)\n",
        "\n",
        "  def forward(self, decoder_input, encoder_output, self_attn_mask=None, dec_enc_attn_mask=None):\n",
        "    res1 = decoder_input\n",
        "    output1, MMHA = self.MMHA(decoder_input, decoder_input, decoder_input, mask=slf_attn_mask)\n",
        "    output1 = res1 + self.layerNorm(output1)\n",
        "    res2 = output1\n",
        "    output2, MHA = self.MHA(output1, encoder_output, encoder_output, mask=dec_enc_attn_mask)\n",
        "    output2 = res2 + self.layerNorm(output2)\n",
        "    res3 = output2\n",
        "    output3 = self.FFN(output2)\n",
        "    decoder_output = res3 + output3\n",
        "\n",
        "    return decoder_output, MMHA, MHA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu7m-3LeVgaF"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx8rkLNzEDgp"
      },
      "source": [
        "### **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xilqaWUiTlpI"
      },
      "source": [
        "def get_pad_mask(seq, pad_idx):\n",
        "  return (seq != pad_idx).unsqueeze(-2)\n",
        "\n",
        "\n",
        "def get_subsequent_mask(seq):\n",
        "    ''' For masking out the subsequent info. '''\n",
        "  sz_b, len_s = seq.size()\n",
        "  subsequent_mask = (1 - torch.triu(\n",
        "      torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
        "  return subsequent_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heWTVCCbFlGR"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    ''' A sequence to sequence model with attention mechanism. '''\n",
        "\n",
        "    def __init__(\n",
        "            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
        "            d_word_vec=512, d_model=512, d_inner=2048,\n",
        "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            n_src_vocab=n_src_vocab, n_position=n_position,\n",
        "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
        "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
        "            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            n_trg_vocab=n_trg_vocab, n_position=n_position,\n",
        "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
        "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
        "            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)\n",
        "\n",
        "        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
        "\n",
        "    def forward(self, src_seq, trg_seq):\n",
        "\n",
        "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
        "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
        "\n",
        "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
        "        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
        "        seq_logit = self.trg_word_prj(dec_output)\n",
        "        if self.scale_prj:\n",
        "            seq_logit *= self.d_model ** -0.5\n",
        "\n",
        "        return seq_logit.view(-1, seq_logit.size(2))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQoklvwN0pvG"
      },
      "source": [
        "##   ***Train model***\n",
        "\n",
        "\n",
        "\n",
        "# **> Todo**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT8y5CD7kiYo"
      },
      "source": [
        "## ***Evaluation***\n",
        "\n",
        "\n",
        "## > **Todo**\n",
        "\n"
      ]
    }
  ]
}